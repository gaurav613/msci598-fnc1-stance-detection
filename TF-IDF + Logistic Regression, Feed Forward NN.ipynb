{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a497e8",
   "metadata": {},
   "source": [
    "### MSCI 598 - Final Project\n",
    "### Gaurav Mudbhatkal - 20747018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047cac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for data\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ef5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
    "from feature_engineering import word_overlap_features\n",
    "from feature_engineering import clean, normalize_word, get_tokenized_lemmas, remove_stopwords\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from utils.score import report_score, LABELS, score_submission\n",
    "\n",
    "from utils.system import parse_params, check_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49ac08",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65f23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "d = DataSet()\n",
    "folds,hold_out = kfold_split(d,n_folds=10)\n",
    "\n",
    "#dict of headline, bodyid and stance \n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32aaccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4124it [00:16, 248.59it/s]\n",
      "4663it [00:22, 208.67it/s]\n",
      "3783it [00:09, 383.16it/s]\n",
      "3388it [00:10, 314.41it/s]\n",
      "3644it [00:12, 299.38it/s]\n",
      "4644it [00:15, 302.62it/s]\n",
      "3848it [00:16, 239.27it/s]\n",
      "4273it [00:15, 276.52it/s]\n",
      "4039it [00:12, 312.94it/s]\n",
      "3944it [00:11, 333.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_features(stances,dataset,name):\n",
    "    h, b, y = [],[],[]\n",
    "\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\"+name+\".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\"+name+\".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\"+name+\".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\"+name+\".npy\")\n",
    "\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X,y\n",
    "\n",
    "def clean_data(stances, dataset):\n",
    "    h, b, y = [], [], []\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "    X = []\n",
    "    clean_headlines = []\n",
    "    clean_bodies = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(h, b))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        clean_headlines.append(clean_headline)\n",
    "        clean_bodies.append(clean_body)\n",
    "    return clean_headlines, clean_bodies, y\n",
    "\n",
    "clean_headlines_folds = dict()\n",
    "clean_bodies_folds = dict()\n",
    "labels = dict()\n",
    "\n",
    "print(\"Cleaning data\")\n",
    "for fold in fold_stances:\n",
    "    clean_headlines_folds[fold], clean_bodies_folds[fold], labels[fold] = clean_data(fold_stances[fold],d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15fe1f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding features\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding features\")\n",
    "# feature vectors for each fold\n",
    "Xs = dict()\n",
    "ys = dict()\n",
    "for fold in fold_stances:\n",
    "    Xs[fold],ys[fold] = generate_features(fold_stances[fold],d,str(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88920bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(folds)))\n",
    "\n",
    "remaining_features_temp = []\n",
    "for i in ids:\n",
    "    remaining_features_temp.append(Xs[i])\n",
    "remaining_features = [item for sublist in remaining_features_temp for item in sublist]\n",
    "\n",
    "clean_headlines_temp = []\n",
    "for i in ids:\n",
    "    clean_headlines_temp.append(clean_headlines_folds[i])\n",
    "clean_headlines = [item for sublist in clean_headlines_temp for item in sublist]\n",
    "\n",
    "clean_bodies_temp = []\n",
    "for i in ids:\n",
    "    clean_bodies_temp.append(clean_headlines_folds[i])\n",
    "clean_bodies = [item for sublist in clean_bodies_temp for item in sublist]\n",
    "\n",
    "stances_temp = []\n",
    "for i in ids:\n",
    "    stances_temp.append(labels[i])\n",
    "stances = [item for sublist in stances_temp for item in sublist]\n",
    "\n",
    "clean_headlines_sent = []\n",
    "for headline in clean_headlines:\n",
    "    clean_headlines_sent.append(\" \".join(headline))\n",
    "    \n",
    "clean_bodies_sent = []\n",
    "for body in clean_bodies:\n",
    "    clean_bodies_sent.append(\" \".join(body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9d1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-Idf \n",
    "vectorizer_total = feature_extraction.text.TfidfVectorizer(max_features=8000, ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161cbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_total.fit(clean_headlines_sent+clean_bodies_sent)\n",
    "vectorized_headlines = vectorizer_total.transform(clean_headlines_sent)\n",
    "vectorized_bodies = vectorizer_total.transform(clean_bodies_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51575f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\Anaconda3\\envs\\msci598\\lib\\site-packages\\scipy\\spatial\\distance.py:699: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "cosine_sims = []\n",
    "for headline, body in list(zip(clean_headlines_sent, clean_bodies_sent)):\n",
    "    vectorized_headline = vectorizer_total.transform([headline]).toarray()\n",
    "    vectorized_body = vectorizer_total.transform([body]).toarray()\n",
    "#     print(vectorized_headline.shape)\n",
    "#     print(vectorized_body.shape)\n",
    "    cosine_sim = 1 - spatial.distance.cosine(vectorized_headline, vectorized_body) # for similarity, 1-score\n",
    "    cosine_sims.append(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd3187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "from scipy.sparse import hstack\n",
    "cosine_similarity = np.array(cosine_sims).reshape(-1,1)\n",
    "\n",
    "X_train = np.c_[np.array(remaining_features), cosine_similarity]\n",
    "# X_train_arr = X_train.toarray()\n",
    "y_train = to_categorical(stances, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cba308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25413it [01:22, 308.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the competition dataset\n",
    "competition_dataset = DataSet(\"competition_test\")\n",
    "\n",
    "clean_headlines_comp, clean_bodies_comp, labels_comp = clean_data(competition_dataset.stances,competition_dataset)\n",
    "X_competition, y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
    "\n",
    "clean_headlines_sent_comp = []\n",
    "for headline in clean_headlines_comp:\n",
    "    clean_headlines_sent_comp.append(\" \".join(headline))\n",
    "    \n",
    "clean_bodies_sent_comp = []\n",
    "for body in clean_bodies_comp:\n",
    "    clean_bodies_sent_comp.append(\" \".join(body))\n",
    "\n",
    "vectorized_headlines_comp = vectorizer_total.transform(clean_headlines_sent_comp)\n",
    "vectorized_bodies_comp = vectorizer_total.transform(clean_bodies_sent_comp)\n",
    "\n",
    "cosine_sims_comp = []\n",
    "for headline, body in list(zip(clean_headlines_sent_comp, clean_bodies_sent_comp)):\n",
    "    vectorized_headline = vectorizer_total.transform([headline]).toarray()\n",
    "    vectorized_body = vectorizer_total.transform([body]).toarray()\n",
    "    cosine_sim = 1 - spatial.distance.cosine(vectorized_headline, vectorized_body) # similarity: 1-score\n",
    "    cosine_sims_comp.append(cosine_sim)\n",
    "\n",
    "cosine_similarity_comp = np.array(cosine_sims_comp).reshape(-1,1)\n",
    "\n",
    "# X_comp = hstack((vectorized_headlines_comp, X_competition, cosine_similarity_comp, vectorized_bodies_comp))\n",
    "X_comp = np.c_[X_competition, cosine_similarity_comp]\n",
    "# X_comp_arr = X_comp.toarray()\n",
    "y_comp = to_categorical(labels_comp, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a775ada",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67974b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\Anaconda3\\envs\\msci598\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, stances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d16b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    262    |     5     |   1441    |    195    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    80     |     0     |    465    |    152    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    341    |     2     |   3708    |    413    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    82     |     0     |   1108    |   17159   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8843.25 out of 11651.25\t(75.89958158995816%)\n"
     ]
    }
   ],
   "source": [
    "# classifier.fit(X_train,stances)\n",
    "predicted_logreg = [LABELS[int(a)] for a in clf.predict(X_comp)]\n",
    "actual_logreg = [LABELS[int(a)] for a in labels_comp]\n",
    "\n",
    "# score_submission(actual, predicted)\n",
    "report_score(actual_logreg, predicted_logreg)\n",
    "\n",
    " # write predicted labels to file\n",
    "with open(\"logreg_predictions.csv\",\"w\") as f:\n",
    "    for prediction in predicted_logreg:\n",
    "        f.write(prediction + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356dd4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4629230300628946, 0.47586721058518794)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(actual_logreg, predicted_logreg, average='macro'), recall_score(actual_logreg, predicted_logreg, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0325fb4",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c182bc87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "81/81 [==============================] - 1s 3ms/step - loss: 0.8452 - accuracy: 0.7480\n",
      "Epoch 2/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.8375\n",
      "Epoch 3/100\n",
      "81/81 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8519\n",
      "Epoch 4/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3967 - accuracy: 0.8600\n",
      "Epoch 5/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8630\n",
      "Epoch 6/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8652\n",
      "Epoch 7/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8658\n",
      "Epoch 8/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.8665\n",
      "Epoch 9/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8674\n",
      "Epoch 10/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8680\n",
      "Epoch 11/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.8678\n",
      "Epoch 12/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8688\n",
      "Epoch 13/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8687\n",
      "Epoch 14/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8692\n",
      "Epoch 15/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8701\n",
      "Epoch 16/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8694\n",
      "Epoch 17/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8707\n",
      "Epoch 18/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8705\n",
      "Epoch 19/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.8714\n",
      "Epoch 20/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8710\n",
      "Epoch 21/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8711\n",
      "Epoch 22/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8727\n",
      "Epoch 23/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8728\n",
      "Epoch 24/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8723\n",
      "Epoch 25/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8719\n",
      "Epoch 26/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8731\n",
      "Epoch 27/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8728\n",
      "Epoch 28/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8723\n",
      "Epoch 29/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8736\n",
      "Epoch 30/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8732\n",
      "Epoch 31/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8738\n",
      "Epoch 32/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8744\n",
      "Epoch 33/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8735\n",
      "Epoch 34/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8729\n",
      "Epoch 35/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8747\n",
      "Epoch 36/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8737\n",
      "Epoch 37/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8746\n",
      "Epoch 38/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8750\n",
      "Epoch 39/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8753\n",
      "Epoch 40/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3435 - accuracy: 0.8750\n",
      "Epoch 41/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8758\n",
      "Epoch 42/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8761\n",
      "Epoch 43/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8753\n",
      "Epoch 44/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8754\n",
      "Epoch 45/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8760\n",
      "Epoch 46/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8748\n",
      "Epoch 47/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8761\n",
      "Epoch 48/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8762\n",
      "Epoch 49/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8771\n",
      "Epoch 50/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8762\n",
      "Epoch 51/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8768\n",
      "Epoch 52/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8772\n",
      "Epoch 53/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8772\n",
      "Epoch 54/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8765\n",
      "Epoch 55/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8766\n",
      "Epoch 56/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8777\n",
      "Epoch 57/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8774\n",
      "Epoch 58/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.8771\n",
      "Epoch 59/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8781\n",
      "Epoch 60/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8773\n",
      "Epoch 61/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8780\n",
      "Epoch 62/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8768\n",
      "Epoch 63/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8780\n",
      "Epoch 64/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8783\n",
      "Epoch 65/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8788\n",
      "Epoch 66/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8786\n",
      "Epoch 67/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3369 - accuracy: 0.8784\n",
      "Epoch 68/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8784\n",
      "Epoch 69/100\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8779\n",
      "Epoch 70/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8781\n",
      "Epoch 71/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8789\n",
      "Epoch 72/100\n",
      "81/81 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.8794\n",
      "Epoch 73/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8786\n",
      "Epoch 74/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8774\n",
      "Epoch 75/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8792\n",
      "Epoch 76/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8788\n",
      "Epoch 77/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8782\n",
      "Epoch 78/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8783\n",
      "Epoch 79/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8785\n",
      "Epoch 80/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8786\n",
      "Epoch 81/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3359 - accuracy: 0.8783\n",
      "Epoch 82/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3351 - accuracy: 0.8788\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8780\n",
      "Epoch 84/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8790\n",
      "Epoch 85/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8800\n",
      "Epoch 86/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8790\n",
      "Epoch 87/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8786\n",
      "Epoch 88/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8789\n",
      "Epoch 89/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.8789\n",
      "Epoch 90/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8791\n",
      "Epoch 91/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8785\n",
      "Epoch 92/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8783\n",
      "Epoch 93/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.8794\n",
      "Epoch 94/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.8795\n",
      "Epoch 95/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8790\n",
      "Epoch 96/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8782\n",
      "Epoch 97/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8789\n",
      "Epoch 98/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3331 - accuracy: 0.8795\n",
      "Epoch 99/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8794\n",
      "Epoch 100/100\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dropout(0.6))\n",
    "model.add(layers.Dense(64,  activation='relu'))\n",
    "# model.add(layers.Dense(32, activation='softmax'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3052938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    123    |     2     |   1580    |    198    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    22     |     0     |    506    |    169    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    138    |     0     |   3900    |    426    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    14     |     0     |   1120    |   17215   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8888.75 out of 11651.25\t(76.29009762900976%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.29009762900976"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_dense = [LABELS[int(np.argmax(a))] for a in model.predict(X_comp)]\n",
    "actual_dense = [LABELS[int(a)] for a in labels_comp]\n",
    "\n",
    "# score_submission(actual, predicted)\n",
    "report_score(actual_dense, predicted_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6970bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_dense.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d751b12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4797343507787215, 0.46912224202304054)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(actual_dense, predicted_dense, average='macro'), recall_score(actual_dense, predicted_dense, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407723f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # write predicted labels to file\n",
    "with open(\"dense_predictions.csv\",\"w\") as f:\n",
    "    for prediction in predicted_dense:\n",
    "        f.write(prediction + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
